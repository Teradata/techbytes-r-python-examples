{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# The contents of this file are Teradata Public Content and have been released\n",
    "# to the Public Domain.\n",
    "# Tim Miller & Alexander Kolovos & Pankaj Vinod Purandare - Apr. 2020 - v.1.1\n",
    "# Copyright (c) 2020 by Teradata\n",
    "# Licensed under BSD; see \"license.txt\" file in the bundle root folder.\n",
    "#\n",
    "################################################################################\n",
    "# R and Python TechBytes Demo - Part 3: teradataml\n",
    "# ------------------------------------------------------------------------------\n",
    "# File: R_Py_TechBytes-Part_3-Demo.py\n",
    "# ------------------------------------------------------------------------------\n",
    "# The R and Python TechBytes Demo comprises of 5 parts:\n",
    "# Part 1 consists of only a Powerpoint overview of R and Python in Vantage\n",
    "# Part 2 demonstrates the Teradata R package tdplyr for clients\n",
    "# Part 3 demonstrates the Teradata Python package teradataml for clients\n",
    "# Part 4 demonstrates using R in-nodes with the SCRIPT and ExecR Table Operators\n",
    "# Part 5 demonstrates using Python in-nodes with the SCRIPT Table Operator\n",
    "################################################################################\n",
    "#\n",
    "# This TechBytes demo utilizes a use case to predict the propensity of a\n",
    "# financial services customer base to open a credit card account.\n",
    "#\n",
    "# In Section 1, Various features will be generated by joining and aggregating\n",
    "# from 3 tables based tables (10K customers, 100K accounts, 1M+ transactions)\n",
    "# into an analytic data set. We will show how to use teradataml functions to do\n",
    "# the following:\n",
    "#\n",
    "#   a) Pull through the cust_id, income, age, years_with_bank, nbr_children\n",
    "#      from the Customer table\n",
    "#   b) Create a gender indicator variable (female_ind) from gender in the\n",
    "#      Customer table\n",
    "#   c) Create marital status indicator variables (single_ind, married_ind,\n",
    "#      seperated_ind) from marital_status in the Customer table\n",
    "#   d) Create location indicator variables (ca_resident, ny_resident,\n",
    "#      tx_resident, il_resident, az_resident, oh_resident) from state_code in\n",
    "#      Customer table\n",
    "#   e) Create account indicator variables (ck_acct_ind, cc_acct_ind,\n",
    "#      sv_acct_ind) from acct_type in the Account table\n",
    "#   f) Create average balance variables (ck_avg_bal, cc_avg_bal, sv_avg_bal)\n",
    "#      by taking the mean of the beginning_balance and ending_balance in the\n",
    "#      Account table\n",
    "#   g) Create average transaction amounts (ck_avg_tran_amt, cc_avg_tran_amt,\n",
    "#      sv_avg_tran_amt) by taking the average of the principal_amt and\n",
    "#      interest_amt in the Transactions table\n",
    "#   h) Create quarterly transaction counts (q1_nbr_trans, q2_nbr_trans,\n",
    "#      q3_nbr_trans, q4_nbr_trans) by taking the count of tran_id's based\n",
    "#      upon tran_date in the Transactions table\n",
    "#\n",
    "# In Section 2, we create an XGBoost model and a Decision Forest model on a\n",
    "# 60% sample of rows.\n",
    "# Furthermore, we test/score both models with the remaining 40%.\n",
    "# We conclude the present demo Part 3 with the following operations:\n",
    "# - Run decision forest evaluator to determine the most pertinent variables.\n",
    "# - Run confusion matrix on both scored data sets.\n",
    "# - Save the models so that they can be scored again in the future.\n",
    "#\n",
    "# Note: Code executed successfully on Python v.3.6.8, and by using teradataml\n",
    "#       v.16.20.00.05 to connect to a Vantage system that runs Advanced SQL\n",
    "#       Engine database v.16.20.40.01.\n",
    "################################################################################\n",
    "# File Changelog\n",
    "#  v.1.0     2019-10-29     First release\n",
    "#  v.1.1     2020-04-02     Code simplified with case, sample teradataml funcs.\n",
    "#                           Additional information about connections.\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load teradataml and dependency packages\n",
    "from teradataml import create_context, DataFrame, get_context, copy_to_sql, in_schema\n",
    "from teradataml.dataframe.sql_functions import case\n",
    "from teradataml import XGBoost, XGBoostPredict, DecisionForest, DecisionForestEvaluator, DecisionForestPredict, ConfusionMatrix\n",
    "from sqlalchemy.sql.expression import select, or_, extract, text, join, case as case_when\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that is used later in the demo code.\n",
    "def draw_box_plot(data, plotColumnName, xTicksColumnName, xLabel = None, yLabel = None, title = None):\n",
    "    \"\"\"\n",
    "    DESCRIPTION:\n",
    "        Function to display bar plot using the data based on the given parameters.\n",
    "\n",
    "    PARAMETERS:\n",
    "        data:\n",
    "            Required argument. A nx2 pandas dataframe with one column containing\n",
    "            numeric values.\n",
    "\n",
    "        plotColumnName:\n",
    "            Required argument. Column name of the pandas dataframe that contains\n",
    "            numeric values.\n",
    "\n",
    "        xTicksColumnName:\n",
    "            Required argument. Other column name for which count is given in\n",
    "            plotColumnName column values.\n",
    "\n",
    "        xLabel:\n",
    "            Optional argument. Name of the label on X-axis\n",
    "\n",
    "        yLabel:\n",
    "            Optional argument. Name of the label on Y-axis\n",
    "\n",
    "        title:\n",
    "            Optional argument. Title of the bar plot\n",
    "\n",
    "    RETURNS:\n",
    "        None\n",
    "    \"\"\"\n",
    "    index = np.arange(data.shape[0])\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.grid()\n",
    "    ax.bar(index, data[plotColumnName])\n",
    "    plt.xlabel(xLabel) if xLabel is not None else plt.xlabel(xTicksColumnName)\n",
    "    plt.ylabel(yLabel) if yLabel is not None else plt.ylabel(\"Count\")\n",
    "    plt.xticks(index, data[xTicksColumnName], rotation=30)\n",
    "    plt.title(title) if title is not None else plt.title(\"Bar Plot\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection to Teradata Vantage server (uses the Teradata SQL Driver\n",
    "# for Python). Before you execute the following statement, replace the variables\n",
    "# <HOSTNAME>, <UID> and <PWD> with your target Vantage system hostname (or\n",
    "# IP address), and your database user ID and password, respectively.\n",
    "td_context = create_context(host=\"<HOSTNAME>\", username=\"<UID>\", password=\"<PWD>\")\n",
    "\n",
    "# Notes and alternatives:\n",
    "# 1. In any connection function, you can specify for an argument the getpass()\n",
    "#    function of the Python standard library. First, you will need to execute:\n",
    "#    import getpass\n",
    "#    getpass() enables you to type your password secretly during runtime\n",
    "#    without having to hard-code it in the script.\n",
    "# Example: Specifying the argument password = getpass.getpass(\"Password: \") will\n",
    "#          produce a prompt string \"Password: \" that expects you to type in\n",
    "#          a password to proceed.\n",
    "# 2. Specifying the optional argument logmech enables you to specify particular\n",
    "#    logging mechanisms that may apply on your target server. For example, you\n",
    "#    can connect via an active directory with LDAP credentials by specifying\n",
    "#    the argument: logmech = \"LDAP\" as follows:\n",
    "# td_context = create_context(host=\"<HOSTNAME>\", username=\"<UID>\", password=\"PWD\", logmech=\"LDAP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Data manipulation and transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this section, we start with our initial input tables and use teradataml\n",
    "# to perform a series of operations on the data. The goal is to produce an\n",
    "# Analytic Data Set (ADS) with the features we need for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames for the Customer, Accounts and Transactions tables in the\n",
    "# Vantage Advanced SQL Engine. Before you execute the following statements, \n",
    "# replace the variable <DBNAME> with the target Vantage system database name\n",
    "# where the corresponding table resides.\n",
    "# Note: Use the in_schema() function only if tables reside in a database\n",
    "#       <DBNAME> other than the default database of the connected user.\n",
    "tdCustomer = DataFrame(in_schema(\"<DBNAME>\", \"Customer\"))\n",
    "# Using to_pandas() for a cleaner display format\n",
    "tdCustomer.to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdAccounts = DataFrame(in_schema(\"<DBNAME>\", \"Accounts\"))\n",
    "# Using to_pandas() for a cleaner display format\n",
    "tdAccounts.to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdTransactions = DataFrame(in_schema(\"<DBNAME>\", \"Transactions\"))\n",
    "# Using to_pandas() for a cleaner display format\n",
    "tdTransactions.to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis: Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Histogram plot of income\n",
    "\n",
    "tdCustomer_hist_pd = tdCustomer[tdCustomer.income != None].to_pandas()\n",
    "# Note: to_pandas() converts the Teradata \"decimal.Decimal\" type into the Python\n",
    "#       non-numeric \"object\" type. For histogram, the column should be numeric\n",
    "#       type. Hence, we use astype() to convert the data type of column 'income'\n",
    "#       of pandas DataFrame.\n",
    "tdCustomer_hist_pd['income'] = tdCustomer_hist_pd['income'].astype('float64')\n",
    "tdCustomer_hist_pd.hist(column=\"income\", bins=30)\n",
    "plt.xlabel(\"income\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.title(\"Histogram of column 'income'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bar plot: Number of customers gender-wise\n",
    "\n",
    "tdCustomer_bar_pd = tdCustomer.groupby(\"gender\").count().select(['gender','count_cust_id']).to_pandas()\n",
    "draw_box_plot(data = tdCustomer_bar_pd, plotColumnName = 'count_cust_id', xTicksColumnName = 'gender',\n",
    "              xLabel = \"Gender\", yLabel = \"No of customers\", title = \"Gender-wise customers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bar plot: Number of customers state-wise\n",
    "\n",
    "tdCustomer_bar_pd = tdCustomer.groupby(\"state_code\").count().select(['state_code','count_cust_id']).sort(['state_code']).to_pandas()\n",
    "draw_box_plot(data = tdCustomer_bar_pd, plotColumnName = 'count_cust_id', xTicksColumnName = 'state_code',\n",
    "              xLabel = \"State ID\", yLabel = \"No of customers\", title = \"State-wise customers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bar plot: Number of customers Marital status-wise\n",
    "\n",
    "tdCustomer_bar_pd = tdCustomer.groupby(\"marital_status\").count().select(['marital_status','count_cust_id']).sort(['marital_status']).to_pandas()\n",
    "draw_box_plot(data = tdCustomer_bar_pd, plotColumnName = 'count_cust_id', xTicksColumnName = 'marital_status',\n",
    "              xLabel = \"Marital Status\", yLabel = \"No of customers\", title = \"Marital status-wise customers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Scatter plot: Starting and ending balance\n",
    "\n",
    "tdAccounts_pd = tdAccounts[tdAccounts.ending_balance != None].to_pandas()\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.grid()\n",
    "ax.scatter(tdAccounts_pd['starting_balance'], tdAccounts_pd['ending_balance'], s = 3)\n",
    "plt.xlabel(\"starting_balance\")\n",
    "plt.ylabel(\"ending_balance\")\n",
    "plt.title(\"Scatter plot of starting_balance and ending_balance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, grab customer demographic variables. Use imported case() function to\n",
    "# create indicator variables for gender, marital_status and the state_code\n",
    "# (we consider a classification into the top 6 states and the jointly the rest).\n",
    "cust = tdCustomer.assign(female = case([(tdCustomer.gender == \"F\", 1)], else_ = 0 ),\n",
    "                         single    = case( [(tdCustomer.marital_status == 1, 1)], else_ = 0 ),\n",
    "                         married   = case( [(tdCustomer.marital_status == 2, 1)], else_ = 0 ),\n",
    "                         separated = case( [(tdCustomer.marital_status == 3, 1)], else_ = 0 ),\n",
    "                         ca_resident = case( [(tdCustomer.state_code == \"CA\", 1)], else_ = 0 ),\n",
    "                         ny_resident = case( [(tdCustomer.state_code == \"NY\", 1)], else_ = 0 ),\n",
    "                         tx_resident = case( [(tdCustomer.state_code == \"TX\", 1)], else_ = 0 ),\n",
    "                         il_resident = case( [(tdCustomer.state_code == \"IL\", 1)], else_ = 0 ),\n",
    "                         az_resident = case( [(tdCustomer.state_code == \"AZ\", 1)], else_ = 0 ),\n",
    "                         oh_resident = case( [(tdCustomer.state_code == \"OH\", 1)], else_ = 0 )\n",
    "                        )\n",
    "cust.to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next: Get the account information required for the aggregation, and create\n",
    "# the indicator variables for acct_type.\n",
    "acct_balance = tdAccounts.starting_balance + tdAccounts.ending_balance\n",
    "acct = tdAccounts.assign(ck_acct = case( [(tdAccounts.acct_type == \"CK\", 1)], else_ = 0 ),\n",
    "                         sv_acct = case( [(tdAccounts.acct_type == \"SV\", 1)], else_ = 0 ),\n",
    "                         cc_acct = case( [(tdAccounts.acct_type == \"CC\", 1)], else_ = 0 ),\n",
    "                         ck_bal = case( [(tdAccounts.acct_type == \"CK\", acct_balance.expression)], else_ = 0 ),\n",
    "                         sv_bal = case( [(tdAccounts.acct_type == \"SV\", acct_balance.expression)], else_ = 0 ),\n",
    "                         cc_bal = case( [(tdAccounts.acct_type == \"CC\", acct_balance.expression)], else_ = 0 )\n",
    "                        )\n",
    "acct.to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next: Get the transaction information required for the aggregation. Pull out\n",
    "# the quarter the transaction was made.\n",
    "acct_mon = extract('month', tdTransactions.tran_date.expression).expression\n",
    "trans = tdTransactions.assign(q1_trans = case( [(acct_mon ==  \"1\", 1), (acct_mon ==  \"2\", 1), (acct_mon ==  \"3\", 1)], else_ = 0 ),\n",
    "                              q2_trans = case( [(acct_mon ==  \"4\", 1), (acct_mon ==  \"5\", 1), (acct_mon ==  \"6\", 1)], else_ = 0 ),\n",
    "                              q3_trans = case( [(acct_mon ==  \"7\", 1), (acct_mon ==  \"8\", 1), (acct_mon ==  \"9\", 1)], else_ = 0 ),\n",
    "                              q4_trans = case( [(acct_mon == \"10\", 1), (acct_mon == \"11\", 1), (acct_mon == \"12\", 1)], else_ = 0 ),\n",
    "                             )\n",
    "trans.to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Finally, pull everything together into an analytic data set\n",
    "\n",
    "# Initially, we wish to left join the acct and trans on 'acct_nbr' into \n",
    "# an acct_trans DataFrame.\n",
    "acct_trans_cols = ['cust_id', 'acct_type', 'starting_balance', 'ending_balance',\n",
    "                   'acct_acct_nbr', 'principal_amt', 'interest_amt', 'tran_id',\n",
    "                   'tran_date', 'q1_trans', 'q2_trans', 'q3_trans', 'q4_trans',\n",
    "                   'cc_acct', 'cc_bal', 'ck_acct', 'ck_bal', 'sv_acct', 'sv_bal']\n",
    "\n",
    "acct_trans_tmp = acct.join(other = trans,\n",
    "                           on = [acct.acct_nbr == trans.acct_nbr],\n",
    "                           how = \"left\", lsuffix = \"acct\", rsuffix = \"trans\").select(acct_trans_cols)\n",
    "\n",
    "# We create the target acct_trans DataFrame by also adding new columns with the\n",
    "# transaction amounts for each type of account, according to the following.\n",
    "acct_trans_amt = trans.principal_amt + trans.interest_amt\n",
    "\n",
    "acct_trans = acct_trans_tmp.assign(\n",
    "                 ck_tran_amt = case( [(acct_trans_tmp.acct_type == \"CK\", acct_trans_amt.expression)], else_ = 0 ),\n",
    "                 sv_tran_amt = case( [(acct_trans_tmp.acct_type == \"SV\", acct_trans_amt.expression)], else_ = 0 ),\n",
    "                 cc_tran_amt = case( [(acct_trans_tmp.acct_type == \"CC\", acct_trans_amt.expression)], else_ = 0 )\n",
    "                                  )\n",
    "acct_trans.to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, perform a left outer join of acct_trans with cust on 'cust_id'.\n",
    "# This time, obtain the resulting table by dropping all the temporary table\n",
    "# columns and assigning from scratch the ones we want. For select columns,\n",
    "# use the case() function to replace any None occurrences with zeros.\n",
    "ADS_Py_join_tmp = cust.join(other = acct_trans,\n",
    "                            on = [cust.cust_id == acct_trans.cust_id],\n",
    "                            how = \"left\", lsuffix = \"cust\", rsuffix = \"actr\")\n",
    "\n",
    "ADS_Py_join = ADS_Py_join_tmp.assign(drop_columns = True,\n",
    "                  cust_id = ADS_Py_join_tmp.cust_cust_id,\n",
    "                  income = ADS_Py_join_tmp.income,\n",
    "                  age = ADS_Py_join_tmp.age,\n",
    "                  gender = ADS_Py_join_tmp.gender,\n",
    "                  years_with_bank = ADS_Py_join_tmp.years_with_bank,\n",
    "                  nbr_children = ADS_Py_join_tmp.nbr_children,\n",
    "                  marital_status = ADS_Py_join_tmp.marital_status,\n",
    "                  state_code = ADS_Py_join_tmp.state_code,\n",
    "                  female = ADS_Py_join_tmp.female,\n",
    "                  single = ADS_Py_join_tmp.single,\n",
    "                  married = ADS_Py_join_tmp.married,\n",
    "                  separated = ADS_Py_join_tmp.separated,\n",
    "                  ca_resident = ADS_Py_join_tmp.ca_resident,\n",
    "                  ny_resident = ADS_Py_join_tmp.ny_resident,\n",
    "                  tx_resident = ADS_Py_join_tmp.tx_resident,\n",
    "                  il_resident = ADS_Py_join_tmp.il_resident,\n",
    "                  az_resident = ADS_Py_join_tmp.az_resident,\n",
    "                  oh_resident = ADS_Py_join_tmp.oh_resident,\n",
    "                  acct_type = ADS_Py_join_tmp.acct_type,\n",
    "                  starting_balance = ADS_Py_join_tmp.starting_balance,\n",
    "                  ending_balance = ADS_Py_join_tmp.ending_balance,\n",
    "                  acct_nbr = ADS_Py_join_tmp.acct_acct_nbr,\n",
    "                  principal_amt = ADS_Py_join_tmp.principal_amt,\n",
    "                  interest_amt = ADS_Py_join_tmp.interest_amt,\n",
    "                  tran_id = ADS_Py_join_tmp.tran_id,\n",
    "                  tran_date = ADS_Py_join_tmp.tran_date,\n",
    "                  q1_trans = case( [(ADS_Py_join_tmp.q1_trans == None, 0)], else_ = ADS_Py_join_tmp.q1_trans ),\n",
    "                  q2_trans = case( [(ADS_Py_join_tmp.q2_trans == None, 0)], else_ = ADS_Py_join_tmp.q2_trans ),\n",
    "                  q3_trans = case( [(ADS_Py_join_tmp.q2_trans == None, 0)], else_ = ADS_Py_join_tmp.q2_trans ),\n",
    "                  q4_trans = case( [(ADS_Py_join_tmp.q2_trans == None, 0)], else_ = ADS_Py_join_tmp.q2_trans ),\n",
    "                  ck_acct = case( [(ADS_Py_join_tmp.ck_acct == None, 0)], else_ = ADS_Py_join_tmp.ck_acct ),\n",
    "                  sv_acct = case( [(ADS_Py_join_tmp.sv_acct == None, 0)], else_ = ADS_Py_join_tmp.sv_acct ),\n",
    "                  cc_acct = case( [(ADS_Py_join_tmp.cc_acct == None, 0)], else_ = ADS_Py_join_tmp.cc_acct ),\n",
    "                  ck_bal = case( [(ADS_Py_join_tmp.ck_bal == None, 0)], else_ = ADS_Py_join_tmp.ck_bal ),\n",
    "                  sv_bal = case( [(ADS_Py_join_tmp.sv_bal == None, 0)], else_ = ADS_Py_join_tmp.sv_bal ),\n",
    "                  cc_bal = case( [(ADS_Py_join_tmp.cc_bal == None, 0)], else_ = ADS_Py_join_tmp.cc_bal ),\n",
    "                  ck_tran_amt = case( [(ADS_Py_join_tmp.ck_tran_amt == None, 0)], else_ = ADS_Py_join_tmp.ck_tran_amt ),\n",
    "                  sv_tran_amt = case( [(ADS_Py_join_tmp.sv_tran_amt == None, 0)], else_ = ADS_Py_join_tmp.sv_tran_amt ),\n",
    "                  cc_tran_amt = case( [(ADS_Py_join_tmp.cc_tran_amt == None, 0)], else_ = ADS_Py_join_tmp.cc_tran_amt )\n",
    "                                    )\n",
    "ADS_Py_join.to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a last step, aggregate and roll up by 'cust_id' all variables in the above join operation.\n",
    "#\n",
    "# This pulls everything together into the analytic data set.\n",
    "ADS_Py = ADS_Py_join.groupby(\"cust_id\").agg(\n",
    "    {\n",
    "        \"income\" : \"min\",\n",
    "        \"age\" : \"min\",\n",
    "        \"years_with_bank\" : \"min\",\n",
    "        \"nbr_children\" : \"min\",\n",
    "        \"female\" : \"min\",\n",
    "        \"single\" : \"min\",\n",
    "        \"married\" : \"min\",\n",
    "        \"separated\" : \"min\",\n",
    "        \"ca_resident\" : \"max\",\n",
    "        \"ny_resident\" : \"max\",\n",
    "        \"tx_resident\" : \"max\",\n",
    "        \"il_resident\" : \"max\",\n",
    "        \"az_resident\" : \"max\",\n",
    "        \"oh_resident\" : \"max\",\n",
    "        \"ck_acct\" : \"max\",\n",
    "        \"sv_acct\" : \"max\",\n",
    "        \"cc_acct\" : \"max\",\n",
    "        \"ck_bal\" : \"mean\",\n",
    "        \"sv_bal\" : \"mean\",\n",
    "        \"cc_bal\" : \"mean\",\n",
    "        \"ck_tran_amt\" : \"mean\",\n",
    "        \"sv_tran_amt\" : \"mean\",\n",
    "        \"cc_tran_amt\" : \"mean\",\n",
    "        \"q1_trans\" : \"count\",\n",
    "        \"q2_trans\" : \"count\",\n",
    "        \"q3_trans\" : \"count\",\n",
    "        \"q4_trans\" : \"count\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Column names have been affected by aggregation. Assign final names.\n",
    "# Note: In teradataml, the rename() and column methods are unavailable yet.\n",
    "#       We use the assign() method combined with select() to achieve the task.\n",
    "columns = ['cust_id','tot_income','tot_age','tot_cust_years','tot_children','female_ind',\n",
    "           'single_ind', 'married_ind', 'separated_ind', 'ca_resident_ind', 'ny_resident_ind',\n",
    "           'tx_resident_ind','il_resident_ind','az_resident_ind', 'oh_resident_ind',\n",
    "           'ck_acct_ind','sv_acct_ind','cc_acct_ind', 'ck_avg_bal','sv_avg_bal','cc_avg_bal',\n",
    "           'ck_avg_tran_amt','sv_avg_tran_amt','cc_avg_tran_amt','q1_trans_cnt',\n",
    "           'q2_trans_cnt','q3_trans_cnt','q4_trans_cnt']\n",
    "\n",
    "ADS_Py = ADS_Py.assign(drop_columns = True,\n",
    "                       cust_id         = ADS_Py.cust_id,\n",
    "                       tot_income      = ADS_Py.min_income,\n",
    "                       tot_age         = ADS_Py.min_age,\n",
    "                       tot_cust_years  = ADS_Py.min_years_with_bank,\n",
    "                       tot_children    = ADS_Py.min_nbr_children,\n",
    "                       female_ind      = ADS_Py.min_female,\n",
    "                       single_ind      = ADS_Py.min_single,\n",
    "                       married_ind     = ADS_Py.min_married,\n",
    "                       separated_ind   = ADS_Py.min_separated,\n",
    "                       ca_resident_ind = ADS_Py.max_ca_resident,\n",
    "                       ny_resident_ind = ADS_Py.max_ny_resident,\n",
    "                       tx_resident_ind = ADS_Py.max_tx_resident,\n",
    "                       il_resident_ind = ADS_Py.max_il_resident,\n",
    "                       az_resident_ind = ADS_Py.max_az_resident,\n",
    "                       oh_resident_ind = ADS_Py.max_oh_resident,\n",
    "                       ck_acct_ind     = ADS_Py.max_ck_acct,\n",
    "                       sv_acct_ind     = ADS_Py.max_sv_acct,\n",
    "                       cc_acct_ind     = ADS_Py.max_cc_acct,\n",
    "                       ck_avg_bal      = ADS_Py.mean_ck_bal,\n",
    "                       sv_avg_bal      = ADS_Py.mean_sv_bal,\n",
    "                       cc_avg_bal      = ADS_Py.mean_cc_bal,\n",
    "                       ck_avg_tran_amt = ADS_Py.mean_ck_tran_amt,\n",
    "                       sv_avg_tran_amt = ADS_Py.mean_sv_tran_amt,\n",
    "                       cc_avg_tran_amt = ADS_Py.mean_cc_tran_amt,\n",
    "                       q1_trans_cnt    = ADS_Py.count_q1_trans,\n",
    "                       q2_trans_cnt    = ADS_Py.count_q2_trans,\n",
    "                       q3_trans_cnt    = ADS_Py.count_q3_trans,\n",
    "                       q4_trans_cnt    = ADS_Py.count_q4_trans).select(columns)\n",
    "\n",
    "# teradataml DataFrame creates views at the backend which are temporary. At the\n",
    "# end of the context removal, all temporary table/views perish. For this reason,\n",
    "# persist the output of ADS_Py as a table in the Advanced SQL Engine. First, DROP\n",
    "# the ADS_Py table, if it previously exists.\n",
    "try:\n",
    "    get_context().execute(\"DROP TABLE ADS_Py\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "copy_to_sql(ADS_Py, table_name=\"ADS_Py\", if_exists=\"replace\")\n",
    "\n",
    "# Create a DataFrame and take a glimpse at it.\n",
    "tdADS_Py = DataFrame(\"ADS_Py\")\n",
    "tdADS_Py.to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Alternatively, optimized written SQL can be used to create the analytic\n",
    "#       data set, as shown in the following:\n",
    "# Before you execute the following statement, replace the variable <DBNAME> with\n",
    "# the target Vantage system database name where the corresponding table resides.\n",
    "ADS_SQL =\"SELECT \\\n",
    "T1.cust_id  AS cust_id \\\n",
    ",MIN(T1.income) AS tot_income \\\n",
    ",MIN(T1.age) AS tot_age \\\n",
    ",MIN(T1.years_with_bank) AS tot_cust_years \\\n",
    ",MIN(T1.nbr_children) AS tot_children \\\n",
    ",CASE WHEN MIN(T1.marital_status) = 1 THEN 1 ELSE 0 END AS single_ind \\\n",
    ",CASE WHEN MIN(T1.gender) = 'F' THEN 1 ELSE 0 END AS female_ind \\\n",
    ",CASE WHEN MIN(T1.marital_status) = 2 THEN 1 ELSE 0 END AS married_ind \\\n",
    ",CASE WHEN MIN(T1.marital_status) = 3 THEN 1 ELSE 0 END AS separated_ind \\\n",
    ",MAX(CASE WHEN T1.state_code = 'CA' THEN 1 ELSE 0 END) AS ca_resident_ind \\\n",
    ",MAX(CASE WHEN T1.state_code = 'NY' THEN 1 ELSE 0 END) AS ny_resident_ind \\\n",
    ",MAX(CASE WHEN T1.state_code = 'TX' THEN 1 ELSE 0 END) AS tx_resident_ind \\\n",
    ",MAX(CASE WHEN T1.state_code = 'IL' THEN 1 ELSE 0 END) AS il_resident_ind \\\n",
    ",MAX(CASE WHEN T1.state_code = 'AZ' THEN 1 ELSE 0 END) AS az_resident_ind \\\n",
    ",MAX(CASE WHEN T1.state_code = 'OH' THEN 1 ELSE 0 END) AS oh_resident_ind \\\n",
    ",MAX(CASE WHEN T2.acct_type = 'CK' THEN 1 ELSE 0 END) AS ck_acct_ind \\\n",
    ",MAX(CASE WHEN T2.acct_type = 'SV' THEN 1 ELSE 0 END) AS sv_acct_ind \\\n",
    ",MAX(CASE WHEN T2.acct_type = 'CC' THEN 1 ELSE 0 END) AS cc_acct_ind \\\n",
    ",AVG(CASE WHEN T2.acct_type = 'CK' THEN T2.starting_balance+T2.ending_balance ELSE 0 END) AS ck_avg_bal \\\n",
    ",AVG(CASE WHEN T2.acct_type = 'SV' THEN T2.starting_balance+T2.ending_balance ELSE 0 END) AS sv_avg_bal \\\n",
    ",AVG(CASE WHEN T2.acct_type = 'CC' THEN T2.starting_balance+T2.ending_balance ELSE 0 END) AS cc_avg_bal \\\n",
    ",AVG(CASE WHEN T2.acct_type = 'CK' THEN T3.principal_amt+T3.interest_amt ELSE 0 END) AS ck_avg_tran_amt \\\n",
    ",AVG(CASE WHEN T2.acct_type = 'SV' THEN T3.principal_amt+T3.interest_amt ELSE 0 END) AS sv_avg_tran_amt \\\n",
    ",AVG(CASE WHEN T2.acct_type = 'CC' THEN T3.principal_amt+T3.interest_amt ELSE 0 END) AS cc_avg_tran_amt \\\n",
    ",COUNT(CASE WHEN ((EXTRACT(MONTH FROM T3.tran_date) + 2) / 3) = 1 THEN T3.tran_id ELSE NULL END) AS q1_trans_cnt \\\n",
    ",COUNT(CASE WHEN ((EXTRACT(MONTH FROM T3.tran_date) + 2) / 3) = 2 THEN T3.tran_id ELSE NULL END) AS q2_trans_cnt \\\n",
    ",COUNT(CASE WHEN ((EXTRACT(MONTH FROM T3.tran_date) + 2) / 3) = 3 THEN T3.tran_id ELSE NULL END) AS q3_trans_cnt \\\n",
    ",COUNT(CASE WHEN ((EXTRACT(MONTH FROM T3.tran_date) + 2) / 3) = 4 THEN T3.tran_id ELSE NULL END) AS q4_trans_cnt \\\n",
    "FROM <DBNAME>.Customer AS T1 \\\n",
    "LEFT OUTER JOIN <DBNAME>.Accounts AS T2 \\\n",
    "ON T1.cust_id = T2.cust_id \\\n",
    "LEFT OUTER JOIN <DBNAME>.Transactions AS T3 \\\n",
    "ON T2.acct_nbr = T3.acct_nbr \\\n",
    "GROUP BY T1.cust_id\"\n",
    "\n",
    "# As earlier, DROP the ADS_Py table, if it previously exists.\n",
    "# In this case, create a DataFrame with from_query() and take a glimpse at it.\n",
    "try:\n",
    "    get_context().execute(\"DROP TABLE ADS_SQL\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "tdADS_SQL = DataFrame.from_query(ADS_SQL)\n",
    "tdADS_SQL.to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Model fitting with the Vantage Analytic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this section, we use the ADS that we created in Section 1 above to perform\n",
    "# model fitting and scoring tasks. We illustrate using teradataml functions that\n",
    "# invoke the XGBoost and Decision Forest algorithms from corresponding analytic\n",
    "# functions that reside in the Vantage Machine Learning Engine of the connected \n",
    "# Vantage system.\n",
    "# Note: To continue into Section 2 of this demo, you must have access\n",
    "#       to a Vantage system with a Machine Learning Engine component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and testing DataSets from the persisted table 'ADS_Py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the analytic data set into training and testing data sets (60/40%)\n",
    "# with the sample() function.\n",
    "\n",
    "ADS_Train_Test = tdADS_Py.sample(frac = [0.60, 0.40])\n",
    "\n",
    "try:\n",
    "    get_context().execute(\"DROP TABLE ADS_Train_Test\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "copy_to_sql(ADS_Train_Test, table_name=\"ADS_Train_Test\", if_exists=\"replace\")\n",
    "\n",
    "# Create a DataFrame and take a glimpse at it.\n",
    "tdTrain_Test = DataFrame(\"ADS_Train_Test\")\n",
    "tdTrain_Test.to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the 60% sample to train.\n",
    "TrainQuery = tdTrain_Test[tdTrain_Test.sampleid == \"1\"]\n",
    "\n",
    "# Use the 40% sample to test/score.\n",
    "TestQuery = tdTrain_Test[tdTrain_Test.sampleid == \"2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and scoring using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, construct a formula to predict Credit Card account ownership based on independent variables of interest:\n",
    "formula = \"cc_acct_ind ~ tot_income + tot_age + tot_cust_years + tot_children + female_ind + single_ind \" \\\n",
    "          \"+ married_ind + separated_ind + ca_resident_ind + ny_resident_ind + tx_resident_ind + il_resident_ind \" \\\n",
    "          \"+ az_resident_ind + oh_resident_ind + ck_acct_ind + sv_acct_ind + ck_avg_bal + sv_avg_bal \" \\\n",
    "          \"+ ck_avg_tran_amt + sv_avg_tran_amt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, train an XGBoost model to predict Credit Card account ownership on the basis of the above formula.\n",
    "td_xgboost_model = XGBoost(data=TrainQuery,\n",
    "                           id_column='cust_id',\n",
    "                           formula = formula,\n",
    "                           num_boosted_trees=4,\n",
    "                           loss_function='binomial',\n",
    "                           prediction_type='classification',\n",
    "                           reg_lambda=1.0,\n",
    "                           shrinkage_factor=0.1,\n",
    "                           iter_num=10,\n",
    "                           min_node_size=1,\n",
    "                           max_depth=10\n",
    "                           )\n",
    "print(td_xgboost_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the XGBoost model against the holdout and compare actuals to predicted.\n",
    "td_xgboost_predict = XGBoostPredict(td_xgboost_model,\n",
    "                                    newdata=TestQuery,\n",
    "                                    object_order_column= ['tree_id','iter','class_num'],\n",
    "                                    id_column='cust_id',\n",
    "                                    terms='cc_acct_ind',\n",
    "                                    num_boosted_trees=4\n",
    "                                    )\n",
    "\n",
    "# Persist the XGBoostPredict output.\n",
    "try:\n",
    "    get_context().execute(\"DROP TABLE XGBoost_Scores\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "td_xgboost_predict.result.to_sql(if_exists = \"replace\", table_name = \"XGBoost_Scores\")\n",
    "tdXGBoost_Scores = DataFrame(\"XGBoost_Scores\")\n",
    "tdXGBoost_Scores.to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and scoring using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a different approach, train a Random Forest model to predict the same target,\n",
    "# so we can compare and see which algorithm fits best the data.\n",
    "# We implement this approach by means of the Decision Forest functions.\n",
    "td_decisionforest_model = DecisionForest(formula = formula,\n",
    "                                         data = TrainQuery,\n",
    "                                         tree_type = \"classification\",\n",
    "                                         ntree = 500,\n",
    "                                         nodesize = 1,\n",
    "                                         variance = 0.0,\n",
    "                                         max_depth = 12,\n",
    "                                         mtry = 5,\n",
    "                                         mtry_seed = 100,\n",
    "                                         seed = 100\n",
    "                                         )\n",
    "print(td_decisionforest_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the DecisionForestEvaluator() function to determine the most important\n",
    "# variables in the Decision Forest model.\n",
    "td_decisionforest_model_evaluator = DecisionForestEvaluator(object = td_decisionforest_model,\n",
    "                                                            num_levels = 5)\n",
    "# In the following, the describe() method provides summary statistics across\n",
    "# the trees over grouping by each variable. One can consider the mean\n",
    "# importance across all trees as the importance for each variable.\n",
    "td_variable_importance = td_decisionforest_model_evaluator.result.select([\"variable_col\", \"importance\"]).groupby(\"variable_col\").describe()\n",
    "print(td_variable_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score the Decision Forest model.\n",
    "td_decisionforest_predict = DecisionForestPredict(td_decisionforest_model,\n",
    "                                                  newdata = TestQuery,\n",
    "                                                  id_column = \"cust_id\",\n",
    "                                                  detailed = False,\n",
    "                                                  terms = [\"cc_acct_ind\"]\n",
    "                                                  )\n",
    "\n",
    "# Persist the DecisionForestPredict output.\n",
    "try:\n",
    "    get_context().execute(\"DROP TABLE RandomForest_Scores\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "copy_to_sql(td_decisionforest_predict.result, if_exists = \"replace\", table_name=\"RandomForest_Scores\")\n",
    "tdRandomForest_Scores = DataFrame(\"RandomForest_Scores\")\n",
    "tdRandomForest_Scores.to_pandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Confusion Matrix to look at the 2 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the confusion matrix for the XGBoost model.\n",
    "confusion_matrix_XGB = ConfusionMatrix(data = tdXGBoost_Scores,\n",
    "                                       reference = \"cc_acct_ind\",\n",
    "                                       prediction = \"prediction\"\n",
    "                                       )\n",
    "print(confusion_matrix_XGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the confusion matrix for Random Forest model.\n",
    "confusion_matrix_DF = ConfusionMatrix(data = tdRandomForest_Scores,\n",
    "                                      reference = \"cc_acct_ind\",\n",
    "                                      prediction = \"prediction\"\n",
    "                                      )\n",
    "print(confusion_matrix_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving models to be reused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Model Administration APIs are not officially supported in teradataml v.16.20.00.05\n",
    "#       and their APIs are not exposed to end user.\n",
    "from teradataml import save_model, list_models, describe_model, delete_model, retrieve_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models so that they can be scored again and managed moving forward.\n",
    "# Expect an error if you ask to save a model with an existing specified name.\n",
    "save_model(td_xgboost_model, name=\"XGBoost_Model_1\")\n",
    "save_model(td_decisionforest_model, name=\"Decision_Forest_Model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show some other methods for model maintenance\n",
    "\n",
    "# List existing saved models.\n",
    "list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide description of a specified model.\n",
    "# Expect an error if a specified model does not exist.\n",
    "describe_model(name = \"Decision_Forest_Model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve saved models and use these in a different session to score.\n",
    "# Expect an error if a specified model does not exist.\n",
    "xgboost_model = retrieve_model(name = \"XGBoost_Model_1\")\n",
    "decision_forest_model = retrieve_model(name=\"Decision_Forest_Model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete a saved model.\n",
    "# Expect an error if a specified model does not exist.\n",
    "delete_model(name=\"XGBoost_Model_1\")\n",
    "delete_model(name=\"Decision_Forest_Model_1\")\n",
    "list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the context of present teradataml session and terminate the Python session.\n",
    "# It is recommended to call the remove_context() function for session cleanup.\n",
    "# Temporary objects are removed at the end of the session.\n",
    "from teradataml import remove_context\n",
    "remove_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
