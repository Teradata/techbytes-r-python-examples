################################################################################
# The contents of this file are Teradata Public Content and have been released
# to the Public Domain.
# Tim Miller & Alexander Kolovos - April 2020 - v.1.1
# Copyright (c) 2020 by Teradata
# Licensed under BSD; see "license.txt" file in the bundle root folder.
#
################################################################################
# R and Python TechBytes Demo - Part 2: tdplyr
# ------------------------------------------------------------------------------
# File: R_Py_TechBytes-Part_2-Demo.r
# ------------------------------------------------------------------------------
# The R and Python TechBytes Demo comprises of 5 parts:
# Part 1 consists of only a Powerpoint overview of R and Python in Vantage
# Part 2 demonstrates the Teradata R package tdplyr for clients
# Part 3 demonstrates the Teradata Python package teradataml for clients
# Part 4 demonstrates using R in-nodes with the SCRIPT and ExecR Table Operators
# Part 5 demonstrates using Python in-nodes with the SCRIPT Table Operator
################################################################################
#
# This TechBytes demo utilizes a use case to predict the propensity of a
# financial services customer base to open a credit card account.
#
# In Section 1, Various features will be generated by joining and aggregating
# from 3 tables based tables (10K customers, 100K accounts, 1M+ transactions)
# into an analytic data set. We will show how to use dplyr verbs to do this,
# as well as raw SQL:
#
#   a) Pull through the cust_id, income, age, years_with_bank, nbr_children
#      from the Customer table
#   b) Create a gender indicator variable (female_ind) from gender in the
#      Customer table
#   c) Create marital status indicator variables (single_ind, married_ind,
#      separated_ind) from marital_status in the Customer table
#   d) Create location indicator variables (ca_resident, ny_resident,
#      tx_resident, il_resident, az_resident, oh_resident) from state_code in
#      Customer table
#   e) Create account indicator variables (ck_acct_ind, cc_acct_ind,
#      sv_acct_ind) from acct_type in the Account table
#   f) Create average balance variables (ck_avg_bal, cc_avg_bal, sv_avg_bal)
#      by taking the mean of the beginning_balance and ending_balance in the
#      Account table
#   g) Create average transaction amounts (ck_avg_tran_amt, cc_avg_tran_amt,
#      sv_avg_tran_amt) by taking the average of the principal_amt and
#      interest_amt in the Transactions table
#   h) Create quarterly transaction counts (q1_nbr_trans, q2_nbr_trans,
#      q3_nbr_trans, q4_nbr_trans) by taking the count of tran_id's based
#      upon tran_date in the Transactions table
#
# In Section 2, we create an XGBoost model and a Decision Forest model on a
# 60% sample of rows.
# Furthermore, we test/score both models with the remaining 40%.
# We conclude the present demo Part 2 with the following operations:
# - Run decision forest evaluator to determine the most pertinent variables.
# - Run confusion matrix on both scored data sets.
# - Save the models so that they can be scored again in the future.
#
# Note: Code executed successfully on R v.3.6.3 running on RStudio v.1.2.5001,
#       and by using tdplyr v.16.20.00.06 to connect to a Vantage system that
#       runs Advanced SQL Engine database v.16.20.40.01.
################################################################################
# File Changelog
#  v.1.0     2019-10-29     First release
#  v.1.1     2020-04-02     Using the tdplyr td_sample() function for sampling.
#                           Fix: L.181: as.Date requires format argument.
#                           Additional information about connections.
################################################################################

# Load tdplyr and dependency packages

LoadPackages <- function() {
library(odbc)
library(DBI)
library(dplyr)
library(dbplyr)
library(teradatasql)
library(tdplyr)
library(dbplot)
}

suppressPackageStartupMessages(LoadPackages())

###
### Connection
###

# Establish a connection to Teradata Vantage server with the Teradata R native
# driver. Before you execute the following statement, replace the variables
# <HOSTNAME>, <UID>, and <PWD> with the target Vantage system hostname, your
# database user ID, and password, respectively.
con <- td_create_context(host = "<HOSTNAME>", dType="native", uid = "<UID>", pwd = "<PWD>")

# With a Teradata R native driver connection, submit a SQL statement explicitly
# to specify a default database <DBNAME>:
dbExecute(con, "DATABASE <DBNAME>")

# Notes and alternatives:
# 1. In any connection function, you can specify for the argument: pwd=getPass()
#    After you specify the statement "library(getPass)", the above argument
#    enables you to type your password secretly during runtime without having
#    to hard-code it in the script.
# 2. Use the dbConnect() function (either in tdplyr or DBI packages).
#    In this approach, you will need to explicitly specify the Teradata R native
#    driver. This alternative also allows you to connect via an active directory
#    with LDAP credentials by also specifying the argument: logmech = "LDAP"
# con <- tdplyr::dbConnect(tdplyr::NativeDriver(), host="<HOSTNAME>", uid="<UID>", pwd="<PWD>")
# 3. Albeit not recommended, you can still use an ODBC-based connection by
#    specifying in the following your target Vantage system DSN name <DSN>,
#    and the <UID>, <PWD>, and <DBNAME> variables appropriately. Specifying the
#    database dbname=<DBNAME> argument is optional.
# con <- DBI::dbConnect(odbc(), dsn="<DSN>", uid="<UID>", pwd="<PWD>", dbname="<DBNAME>")

# Set the execution context.
td_set_context(con)

################################################################################
# Section 1: Data manipulation and transformations
#            In this section, we start with our initial input tables and
#            use tdplyr to perform a series of operations on the data.
#            The goal is to produce an Analytic Data Set (ADS) with the
#            features we need for the analysis.
################################################################################

# Create tibbles for the Customer, Accounts and Transactions tables in the
# Vantage Advanced SQL Engine.

tdCustomer <- tbl(con, "Customer")
glimpse(tdCustomer)
tdAccounts <- tbl(con, "Accounts")
glimpse(tdAccounts)
tdTransactions <- tbl(con, "Transactions")
glimpse(tdTransactions)

###
### Exploratory Data Analysis: Plots
###

# Histograms, bar charts and scatter plots complements of dbplot

tdCustomer %>% filter(!is.na(income)) %>% dbplot_histogram(income)
tdCustomer %>% dbplot_bar(gender)
tdCustomer %>% dbplot_bar(state_code)
tdCustomer %>% dbplot_bar(marital_status)
tdAccounts %>% filter(!is.na(ending_balance)) %>% dbplot_raster(starting_balance, ending_balance, resolution = 250)

###
### Data Pre-Processing
###

# First grab the customer demographic variables and create indicator variables
# for gender, marital_status and state_code (we consider a classification into
# the top 6 states and the jointly the rest).

cust <- tdCustomer %>%
  select(cust_id, income, age, gender, years_with_bank, nbr_children, marital_status, state_code) %>%
  mutate(female = ifelse(gender == 'F', as.integer(1), as.integer(0)),
         single = ifelse(marital_status == '1', as.integer(1), as.integer(0)),
         married = ifelse(marital_status == '2', as.integer(1), as.integer(0)),
         separated = ifelse(marital_status == '3', as.integer(1), as.integer(0)),
         ca_resident = ifelse(state_code == 'CA', as.integer(1), as.integer(0)),
         ny_resident = ifelse(state_code == 'NY', as.integer(1), as.integer(0)),
         tx_resident = ifelse(state_code == 'TX', as.integer(1), as.integer(0)),
         il_resident = ifelse(state_code == 'IL', as.integer(1), as.integer(0)),
         az_resident = ifelse(state_code == 'AZ', as.integer(1), as.integer(0)),
         oh_resident = ifelse(state_code == 'OH', as.integer(1), as.integer(0))
        )

# Next, get the account information required for the aggregation and create
# the indicator variables for acct_type

acct <- tdAccounts %>%
  select(cust_id, acct_type, starting_balance, ending_balance, acct_nbr) %>%
  mutate(ck_acct = ifelse(acct_type == 'CK', as.integer(1), as.integer(0)),
         sv_acct = ifelse(acct_type == 'SV', as.integer(1), as.integer(0)),
         cc_acct = ifelse(acct_type == 'CC', as.integer(1), as.integer(0)))

# Next, get the transaction information required for the aggregation and
# pull out the quarter the transaction was made.

trans <- tdTransactions %>%
  select(acct_nbr, principal_amt, interest_amt, tran_id, tran_date) %>%
  mutate(acct_mon = month(as.Date(tran_date, "yyyy-mm-dd")),
         q1_trans = ifelse(acct_mon %in% c(1,2,3), as.integer(1), as.integer(0)),
         q2_trans = ifelse(acct_mon %in% c(4,5,6), as.integer(1), as.integer(0)),
         q3_trans = ifelse(acct_mon %in% c(7,8,9), as.integer(1), as.integer(0)),
         q4_trans = ifelse(acct_mon %in% c(10,11,12), as.integer(1), as.integer(0)))

# Finally, pull everything together into an analytic data set - Accounts and
# Transactions are LEFT OUTER joined to Customer and all variables must be
# aggregated and rolled up by cust_id

ADS_R <- cust %>%
  left_join(acct, by = "cust_id") %>%
  left_join(trans, by = "acct_nbr") %>%
  group_by(cust_id) %>%
  summarise(tot_income = min(income, na.rm=TRUE),
            tot_age = min(age, na.rm=TRUE),
            tot_cust_years = min(years_with_bank, na.rm=TRUE),
            tot_children = min(nbr_children, na.rm=TRUE),
            female_ind = min(female, na.rm=TRUE),
            single_ind = min(single, na.rm=TRUE),
            married_ind = min(married, na.rm=TRUE),
            separated_ind = min(separated, na.rm=TRUE),
            ca_resident_ind = min(ca_resident, na.rm=TRUE),
            ny_resident_ind = min(ny_resident, na.rm=TRUE),
            tx_resident_ind = min(tx_resident, na.rm=TRUE),
            il_resident_ind = min(il_resident, na.rm=TRUE),
            az_resident_ind = min(az_resident, na.rm=TRUE),
            oh_resident_ind = min(oh_resident, na.rm=TRUE),
            ck_acct_ind = ifelse(is.null(max(ck_acct, na.rm=TRUE)), as.integer(0),
                                 max(ck_acct, na.rm=TRUE)),
            cc_acct_ind = ifelse(is.null(max(cc_acct, na.rm=TRUE)), as.integer(0),
                                 max(cc_acct, na.rm=TRUE)),
            sv_acct_ind = ifelse(is.null(max(sv_acct, na.rm=TRUE)), as.integer(0),
                                 max(sv_acct, na.rm=TRUE)),
            ck_avg_bal = ifelse(is.null(mean(ck_acct*(starting_balance+ending_balance), na.rm=TRUE)), as.integer(0),
                                mean(ck_acct*(starting_balance+ending_balance), na.rm=TRUE)),
            sv_avg_bal = ifelse(is.null(mean(sv_acct*(starting_balance+ending_balance), na.rm=TRUE)), as.integer(0),
                                mean(sv_acct*(starting_balance+ending_balance), na.rm=TRUE)),
            cc_avg_bal = ifelse(is.null(mean(cc_acct*(starting_balance+ending_balance), na.rm=TRUE)), as.integer(0),
                                mean(cc_acct*(starting_balance+ending_balance), na.rm=TRUE)),
            ck_avg_tran_amt = ifelse(is.null(mean(CK_acct*(principal_amt + interest_amt), na.rm=TRUE)), as.integer(0),
                                     mean(CK_acct*(principal_amt + interest_amt), na.rm=TRUE)),
            sv_avg_tran_amt = ifelse(is.null(mean(SV_acct*(principal_amt + interest_amt), na.rm=TRUE)), as.integer(0),
                                     mean(SV_acct*(principal_amt + interest_amt), na.rm=TRUE)),
            cc_avg_tran_amt = ifelse(is.null(mean(CC_acct*(principal_amt + interest_amt), na.rm=TRUE)), as.integer(0),
                                     mean(CC_acct*(principal_amt + interest_amt), na.rm=TRUE)),
            q1_trans_cnt = ifelse(is.null(sum(q1_trans, na.rm=TRUE)), as.integer(0),
                                  sum(q1_trans, na.rm=TRUE)),
            q2_trans_cnt = ifelse(is.null(sum(q2_trans, na.rm=TRUE)), as.integer(0),
                                  sum(q2_trans, na.rm=TRUE)),
            q3_trans_cnt = ifelse(is.null(sum(q3_trans, na.rm=TRUE)), as.integer(0),
                                  sum(q3_trans, na.rm=TRUE)),
            q4_trans_cnt = ifelse(is.null(sum(q4_trans, na.rm=TRUE)), as.integer(0),
                                  sum(q4_trans, na.rm=TRUE))
           )

show_query(ADS_R)

# DROP the ADS table if it exists, and create it in the Advanced SQL Engine.
# Create a tibble and take a glimpse at it.
# Optionally, you can explicity remove an existing table with:
# dbRemoveTable(con, "ADS_R")
# or just use the overwrite=TRUE option to copy_to()

copy_to(con, ADS_R, name="ADS_R", overwrite=TRUE)
tdADS_R <- tbl(con, "ADS_R")
glimpse(tdADS_R)


################################################################################
# Section 2: Model fitting with the Vantage Analytic Functions
#            In this section, we use the ADS that we created in Section 1 above
#            to perform model fitting and scoring tasks. We illustrate using
#            tdplyr functions that invoke the XGBoost and Decision Forest
#            algorithms from corresponding analytic functions that reside in
#            the Vantage Machine Learning Engine of the connected Vantage
#            system.
#            Note: To continue into Section 2 of this demo, you must have access
#                  to a Vantage system with a Machine Learning Engine component.
################################################################################

###
### Create training and testing DataSets from the persisted table 'ADS_R'
###

# Split the data set up into training and testing data sets (60/40%)
ADS_Train_Test <- td_sample(df = tdADS_R, n = c(0.60, 0.40))
copy_to(con, ADS_Train_Test, name="ADS_Train_Test", overwrite=TRUE)

tdTrain_Test <- tbl(con, "ADS_Train_Test")
glimpse(tdTrain_Test)

# Use the 60% sample to train.

TrainQuery <- tdTrain_Test %>% filter(sampleid == "1")

# Use the 40% sample to test/score.

TestQuery <- tdTrain_Test %>% filter(sampleid == "2")

###
### Model training and scoring using XGBoost
###

# Train an XGBoost model to predict Credit Card account ownership based upon
# all independent variables previously created.

td_xgboost_model <- td_xgboost(data=TrainQuery,
                               id.column='cust_id',
                               formula = (cc_acct_ind ~
                                          tot_income +
                                          tot_age +
                                          tot_cust_years +
                                          tot_children +
                                          female_ind +
                                          single_ind +
                                          married_ind +
                                          separated_ind +
                                          ca_resident_ind +
                                          ny_resident_ind +
                                          tx_resident_ind +
                                          il_resident_ind +
                                          az_resident_ind +
                                          oh_resident_ind +
                                          ck_acct_ind +
                                          sv_acct_ind +
                                          ck_avg_bal +
                                          sv_avg_bal +
                                          ck_avg_tran_amt +
                                          sv_avg_tran_amt
                                         ),
                               num.boosted.trees=4,
                               loss.function='binomial',
                               prediction.type='classification',
                               reg.lambda=1,
                               shrinkage.factor=0.1,
                               iter.num=10,
                               min.node.size=1,
                               max.depth=10)

td_xgboost_model

# Score the XGBoost model against the holdout and compare actuals to predicted.

td_xgboost_predict <- predict(td_xgboost_model,
                              newdata=TestQuery,
                              object.order.column= c('tree_id','iter','class_num'),
                              id.column='cust_id',
                              terms='cc_acct_ind',
                              num.boosted.trees=4)

# Persist the XGBoostPredict output.

# Optionally, in the following you can explicitly remove an existing table with:
# dbRemoveTable(con, "XGBoost_Scores")
# or just use the overwrite=TRUE option to copy_to()

copy_to(con, td_xgboost_predict$result, overwrite = TRUE, name = "XGBoost_Scores")
tdXGBoost_Scores <- tbl(con, "XGBoost_Scores")
print(tdXGBoost_Scores, n=100)

###
### Model training and scoring using Random Forest
###

# In a different approach, train a Random Forest model to predict the same
# target, so we can compare and see what algorithm fits the data the best.

td_decisionforest_model <- td_decision_forest(
                           formula = (cc_acct_ind ~
                                      tot_income +
                                      tot_age +
                                      tot_cust_years +
                                      tot_children +
                                      female_ind +
                                      single_ind +
                                      married_ind +
                                      separated_ind +
                                      ca_resident_ind +
                                      ny_resident_ind +
                                      tx_resident_ind +
                                      il_resident_ind +
                                      az_resident_ind +
                                      oh_resident_ind +
                                      ck_acct_ind +
                                      sv_acct_ind +
                                      ck_avg_bal +
                                      sv_avg_bal +
                                      ck_avg_tran_amt +
                                      sv_avg_tran_amt
                           ),
                           data = TrainQuery,
                           tree.type = "classification",
                           ntree = 500,
                           nodesize = 1,
                           variance = 0.0,
                           max.depth = 12,
                           mtry = 5,
                           mtry.seed = 100,
                           seed = 100,
                           data.sequence.column=c("cust_id")
                         )
td_decisionforest_model

# Call td_decision_forest_evaluator to determine the most important variables
# in the Decision Forest model.

td_decisionforest_model_evaluator <- td_decision_forest_evaluator(object=td_decisionforest_model,
                                                                  num.levels=5
)

td_variable_importance <- td_decisionforest_model_evaluator$result %>%
  select(variable_col, importance) %>%
  group_by(variable_col) %>%
  summarise(variable_imp = sum(importance, na.rm=TRUE) / 50) %>%
  arrange(desc(variable_imp))

td_variable_importance

# Score the Decision Forest model

td_decisionforest_predict <- predict(td_decisionforest_model,
                             newdata = TestQuery,
                             id.column = "cust_id",
                             detailed = FALSE,
                             terms = c("cc_acct_ind")
)

# Persist the DecisionForestPredict output.

# Optionally, in the following you can explicitly remove an existing table with:
# dbRemoveTable(con, "RandomForest_Scores")
# or just use the overwrite=TRUE option to copy_to()

copy_to(con, td_decisionforest_predict$result, overwrite = TRUE, name = "RandomForest_Scores")
tdRandomForest_Scores <- tbl(con, "RandomForest_Scores")
print(tdRandomForest_Scores, n=10)

###
### Using Confusion Matrix to look at the 2 models
###

# Look at both models via confusion matrix

confusion_matrix_XGB <- td_confusion_matrix(data = tdXGBoost_Scores,
                                            reference = "cc_acct_ind",
                                            prediction = " prediction"
)

confusion_matrix_XGB

confusion_matrix_DF <- td_confusion_matrix(data = tdRandomForest_Scores,
                                           reference = "cc_acct_ind",
                                           prediction = " prediction"
)

confusion_matrix_DF

###
### Saving models to be reused
###

# Save the models so that they can be scored again and managed moving forward.

# Note: Model Administration APIs are not officially supported in tdplyr
#       v.16.20.00.06 and their APIs are not exposed to end user.

td_save_model(td_xgboost_model, name="XGBoost_Model_1")
td_save_model(td_decisionforest_model, name="Decision_Forest_Model_1")

# Show some other methods for model maintenance

# List existing saved models
td_list_models()

# Provide description of a specified model.
td_describe_model(name = "Decision_Forest_Model_1")

# Delete a saved model.
td_delete_model(name="Decision_Forest_Model_1")

# Use this in another session to retrieve the model

xgboost_model <- td_retrieve_model(name = "XGBoost_Model_1")
xgboost_model
# The following should produce an error, since model has been deleted.
decision_forest_model <- td_retrieve_model(name="Decision_Forest_Model_1")

# Clean-up: Remove the context of present tdplyr connection

td_remove_context()
